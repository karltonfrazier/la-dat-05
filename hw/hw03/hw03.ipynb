{
 "metadata": {
  "name": "",
  "signature": "sha256:7d48d2163d2af01f571d58faa106cfaccad4d3ed594ba4901bec359e481a742a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''For this homework, we will be using the library tweepy to pull data from the Twitter API, \n",
      "and then do language detection on it. That is, you will build a classifier to predict what language \n",
      "a given tweet is, based only on its text.\n",
      "\n",
      "Start by installing tweepy with pip: \n",
      "sudo pip install tweepy \n",
      "\n",
      "Be sure to consult the documentation at http://tweepy.readthedocs.org/en/v3.1.0/index.html \n",
      "read early, read often\n",
      "\n",
      "For the early parts, you can also look at this blog post: \n",
      "http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "In order to access Twitter Streaming API, we need to get 4 pieces of information from Twitter: \n",
      "    API key, \n",
      "    API secret, \n",
      "    Access token, and \n",
      "    Access token secret. \n",
      "Follow the steps below to get all 4 elements:\n",
      "\n",
      "- Create a twitter account if you do not already have one.\n",
      "- Go to https://apps.twitter.com/ and log in with your twitter credentials.\n",
      "- Click \"Create New App\"\n",
      "- Fill out the form, agree to the terms, and click \"Create your Twitter application\"\n",
      "- In the next page, click on \"API keys\" tab, and copy your \"API key\" and \"API secret\".\n",
      "- Scroll down and click \"Create my access token\", and copy your \"Access token\" and \"Access token secret\".\n",
      "\n",
      "Now that that's all set up, let's get get on with the fun stuff!\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To start, let's just get a basic listener set up. All we want to do here is see \n",
      "# a stream of data from Twitter. \n",
      "# If this works, you should see a bunch of json's in your output console. \n",
      "# (If you see anything else -- such as 3-digit numbers or nothing at all -- then it's time to debug.)\n",
      "\n",
      "from tweepy.streaming import StreamListener\n",
      "from tweepy import OAuthHandler\n",
      "from tweepy import Stream\n",
      "\n",
      "#Variables that contains the user credentials to access Twitter API \n",
      "access_token = \"YOUR ACCESS TOKEN\"\n",
      "access_token_secret = \"YOUR ACCESS TOKEN SECRET\"\n",
      "consumer_key = \"YOUR CONSUMER KEY\"\n",
      "consumer_secret = \"YOUR CONSUMER SECRET\"\n",
      "\n",
      "\n",
      "#This is a basic listener that will print incoming data to stdout\n",
      "class StdOutListener(StreamListener):\n",
      "\n",
      "    def on_data(self, data):\n",
      "        print data\n",
      "        return True\n",
      "\n",
      "    def on_error(self, status):\n",
      "        print status\n",
      "\n",
      "\n",
      "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
      "l = StdOutListener()\n",
      "auth = OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "stream = Stream(auth, l)\n",
      "stream.filter(track=['python'])  # only get tweets that have the keyword 'python' in them "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now that that's working, let's filter our data. \n",
      "# This part is actually pretty easy. \n",
      "# Change the on_data method of ListenerParser to extract only the 'text' and 'language' fields, if present.\n",
      "# We also want to be able to retrieve a set number of results, so I have set a max_results parameter \n",
      "# in the constructor. Use that in your edit of on_data to make the object only retrieve up to \n",
      "# max_results many results. \n",
      "\n",
      "#This is a listener that will extract the data we are interested in and print to stdout\n",
      "class ListenerParser(StreamListener):\n",
      "    \n",
      "    def __init__(self, max_results): \n",
      "        super(ListenerParser, self).__init__()\n",
      "        self.texts = []\n",
      "        self.langs = []\n",
      "        if max_results:\n",
      "            self.max_results = max_results\n",
      "        else: \n",
      "            self.max_results = float(\"inf\")\n",
      "    \n",
      "    def on_data(self, data):\n",
      "        \"\"\"YOUR ANSWER HERE\"\"\"\n",
      "\n",
      "    def on_error(self, status):\n",
      "        print status\n",
      "\n",
      "\n",
      "# Now let's get some data! \n",
      "# start with 10 results for testing. \n",
      "# once testing is done, increase to 10,000\n",
      "l = ListenerParser(max_results=10) \n",
      "\n",
      "auth = OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "stream = Stream(auth, l)\n",
      "stream.sample()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check and make sure that you have the same number of texts as tweets. \n",
      "assert len(l.texts) == len(l.langs)\n",
      "print len(l.texts)\n",
      "\n",
      "texts = l.texts \n",
      "langs = l.langs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Awesome, now let's see if we can predict the language using only the text. \n",
      "# Use scikit-learn to split your data into train and test sets, \n",
      "# do the feature extraction on the text, \n",
      "# build 3 different classifiers: Logistic Regression, Naive Bayes, and Random Forest, \n",
      "# and evaluate the results. \n",
      "\n",
      "# First, feature extraction. \n",
      "# Questions: do words or chars make more sense as features here? and what kind of ngram-range? \n",
      "\"\"\"YOUR ANSWER HERE\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Next, split the data into train and test sets. \n",
      "\"\"\"YOUR ANSWER HERE\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Next, some classifiers. \n",
      "# Start with logistic regression. \n",
      "# print a full classification report after you have trained the classifier and made predictions. \n",
      "\"\"\"YOUR ANSWER HERE\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now do the same with multinomial naive bayes \n",
      "\"\"\"YOUR ANSWER HERE\"\"\"\n",
      "# WOW THIS IS FAST"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now let's try to do the random forest \n",
      "\"\"\"YOUR ANSWER HERE\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SPOILER ALERT #\n",
      "# \n",
      "# It broke! \n",
      "# sklearn's random forest does not support sparse input \n",
      "# so, to use a random forest here, we would have to convert to a dense array \n",
      "# let's see how expensive that would be by looking at how many dimensions our data has right now \n",
      "print len(vect.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Wow, that's some very high-dimensional data. \n",
      "# To estimate, if we had 150,000 dimensions * 10,000 datapoints * 4 bytes each (assuming we use int32 or float32; would be 8 byes if we are using int64 or float64)\n",
      "# the dense matrix would take up ~6 GB of memory. \n",
      "# \n",
      "# your laptop probably doesn't have that. \n",
      "# \n",
      "# To deal with this dimensionality problem, we have several options. Among them: \n",
      "# 1. Reduce dimensionality and convert to a dense array \n",
      "# -- use a dimensionality reduction algorithm (but we haven't covered that yet, so that's not really an option)\n",
      "# -- or set a max_features limit in scikit-learn's CountVectorizer\n",
      "# 2. Don't use this classifier \n",
      "# \n",
      "# For now, I'm going to go with 2. \n",
      "# My reasoning is: We already have several classifiers that are very good with high-dimensional data. \n",
      "# Even if we pared our data down to, say, 5000 dimensions using a max_features limit, \n",
      "# we would be throwing away a ton of information, but it would still be pretty tedious \n",
      "# to compute. \n",
      "# \n",
      "# You are free to choose whichever approach you want. \n",
      "# \n",
      "# But as a freebie, I'm going to choose a different classifier that does support sparse input \n",
      "# to use as a third option. This will be a sneak preview of support vector machines.\n",
      "# We will use sklearn's LinearSVC rather than SVC, because it is much faster to train (linear vs. quadratic or even cubic time)\n",
      "# and we don't need any of the extra functionality offered by a nonlinear svm that we could get using SVC."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "\"\"\"YOUR ANSWER HERE\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# With a bit more preprocessing, this score could be substantially higher. \n",
      "# \n",
      "# Moving on, our data is less-than-ideal in a few ways: \n",
      "# 1. Several of the class labels occur only 1 or 2 times; \n",
      "# 2. There are only 10,000 datapoints\n",
      "# 3. We have done almost no preprocessing (of text, or of data after vectorization)\n",
      "# \n",
      "# From here on, this problem set is an open question: How can you improve on these base scores? \n",
      "# What's the best you can produce here? \n",
      "# \n",
      "# Go at it. And record your results in this notebook. \n",
      "# \n",
      "# Notes: \n",
      "# To determine your best score, use a cross validated score with 5 folds. \n",
      "# You are still only allowed to use the text. No meta-data! "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}