{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''First, make sure you have the wikipedia module installed. \n",
      "To make sure you have this, open a terminal and type `sudo pip install wikipedia`.'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "'First, make sure you have the wikipedia module installed. \\nTo make sure you have this, open a terminal and type `sudo pip install wikipedia`.'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Now that that's done, let's get to the good stuff.\n",
      "\n",
      "Look over the short example of what this module can do here: https://pypi.python.org/pypi/wikipedia/\n",
      "\n",
      "It looks like we can get both the content of every page in Wikipedia, as well as every other page \n",
      "that that page links to. That means there is potential for massive, massive datasets within easy reach.\n",
      "\n",
      "Let's investigate by getting the page for \"New York\" as well as all its links.\n",
      "''' \n",
      "\n",
      "import wikipedia \n",
      "\n",
      "ny = wikipedia.page('New York')\n",
      "print(ny.links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ConnectionError",
       "evalue": "HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?srinfo=suggestion&format=json&srsearch=New+York&list=search&srlimit=1&srprop=&limit=1&action=query (Caused by <class 'socket.gaierror'>: [Errno 8] nodename nor servname provided, or not known)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-d09e164d7b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'New York'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/wikipedia/wikipedia.pyc\u001b[0m in \u001b[0;36mpage\u001b[0;34m(title, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuggestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/wikipedia/util.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/wikipedia/wikipedia.pyc\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, results, suggestion)\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0msearch_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'limit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mraw_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wiki_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/wikipedia/wikipedia.pyc\u001b[0m in \u001b[0;36m_wiki_request\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mRATE_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         }\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/njsmwhisper/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ProxyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?srinfo=suggestion&format=json&srsearch=New+York&list=search&srlimit=1&srprop=&limit=1&action=query (Caused by <class 'socket.gaierror'>: [Errno 8] nodename nor servname provided, or not known)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "That's a lot of links! You know what would be cool? Figuring out which articles are similar to others. \n",
      "\n",
      "Your task is to: \n",
      "(1) build a dataset of (at least) 1000 articles; \n",
      "(2) do an LSI on that corpus;\n",
      "(3) cluster the documents in latent-factor space using K-means; \n",
      "(4) retrieve similar documents given an input document.\n",
      "\n",
      "NOTE: IT WILL TAKE SOME TIME TO DOWNLOAD THE DATA FROM WIKIPEDIA. TUNING THE SIZE OF THE DATASET YOU WANT WILL \n",
      "BE A TRADEOFF BETWEEN HOW LONG IT TAKES TO GENERATE THE DATASET, AND HOW RICH AND MEANINGFUL YOUR MODEL IS. \n",
      "If you have the time to just let the program run, it is cool to have a dataset of 10,000 articles. Or 100,000. \n",
      "And if it were realistic, 1,000,000 would be really, really cool. \n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "\"\\nThat's a lot of links! You know what would be cool? Figuring out which articles are similar to others. \\n\\nYour task is to: \\n(1) build a dataset of (at least) 1000 articles; \\n(2) do an LSI on that corpus;\\n(3) cluster the documents in latent-factor space using K-means; \\n(4) retrieve similar documents given an input document.\\n\\nNOTE: IT WILL TAKE SOME TIME TO DOWNLOAD THE DATA FROM WIKIPEDIA. TUNING THE SIZE OF THE DATASET YOU WANT WILL \\nBE A TRADEOFF BETWEEN HOW LONG IT TAKES TO GENERATE THE DATASET, AND HOW RICH AND MEANINGFUL YOUR MODEL IS. \\nIf you have the time to just let the program run, it is cool to have a dataset of 10,000 articles. Or 100,000. \\nAnd if it were realistic, 1,000,000 would be really, really cool. \\n\""
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''First let's get those articles. We will do this by taking a random walk around Wikipedia. \n",
      "As we've just seen, any one article can link to hundreds of articles. But, we would be wise to travel \n",
      "a few links down that chain and drift into topics very different from the original, so that our dataset will be diverse.\n",
      "\n",
      "We will take a random sample of at most 10 links from any given page. \n",
      "\n",
      "We will start our walk with the seed 'Skrillex'. Why? To support local artists.'''\n",
      "\n",
      "import random\n",
      "\n",
      "NUM_LINKS = 10\n",
      "MAX_DOCUMENTS = 1000  ### !IMPORTANT!: MAKING THIS LARGER/SMALLER WILL MAKE YOUR RETRIEVAL TIME LONGER/SHORTER\n",
      "SEED = 'Skrillex'\n",
      "\n",
      "p = wikipedia.page(SEED)\n",
      "pages = {}\n",
      "\n",
      "# choose a random subsample of the links \n",
      "subsample = random.sample(p.links, NUM_LINKS)\n",
      "\n",
      "# I have defined a recursive function to iterate over the graph of links. Read this and understand how it works. \n",
      "def retrieve(list_of_links):\n",
      "    # print the number every 10th document, just to monitor status\n",
      "    if not (len(pages) % 10): \n",
      "        print(len(pages))  \n",
      "    if len(pages) >= MAX_DOCUMENTS: \n",
      "        return pages\n",
      "    for link in list_of_links: \n",
      "        try: \n",
      "            p = wikipedia.page(link)\n",
      "            pages[link] = p.content\n",
      "            subsample = random.sample(p.links, min(len(p.links), NUM_LINKS))  # if a page has less than NUM_LINKS links, then asking for a sample of size MIN_LINKS would throw an error, so we use this min trick\n",
      "            return retrieve(subsample)\n",
      "        except: \n",
      "            continue\n",
      "        \n",
      "pages = retrieve(subsample)\n",
      "print(pages)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Amazing! We have a dataset now. \n",
      "\n",
      "That took a long time. Let's save the dataset in case something goes wrong. (This is generally just a good idea.)'''\n",
      "\n",
      "# save pages in a local file using cPickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Mauri Honkajuuri', u'University of Scranton', u'1943 Boston Braves season', u'IRT Powerhouse', u'1987 National Invitation Tournament', u'Madison Square Garden', u'Creative Review', u'Centaur Media', u'1943 New York Giants (MLB) season', u'Itv.com', u'Keith Olsen', u'Associated British Corporation', u'BMT Broadway Line', u'Tom Paciorek', u'Gaylord Perry', u'Airport Transit System', u'ITV Emergency National Service', u'1890 Cincinnati Reds season', u'Rockaway Park Shuttle', u'Independent Television Service for Wales and the West', u'Platform screen doors', u'Canadian', u'National Register of Historic Places listings in Peekskill, New York', u'African people', u'1894 Cincinnati Reds season', u\"Chicago 'L'\", u'Passionworks', u'Cascade Range', u'Channel Islands of California', u'1935 Chicago Bears season', u'Jethro Tull (band)', u'Xavier University (Cincinnati)', u'Century Motor Vehicle Company', u'Vogue Paris', u'Rockaway Park, Queens', u'Fred Hutchinson', u'Jamaica Estates, Queens', u'South Coast (California)', u'Queensboro Hill, Queens', u'Pennsylvania Route 981', u'Bricolage Production Company', u'Subway Series', u'Bill George (American football player)', u'Fog Bowl (American football)', u'Los Angeles', u'Erie Canal', u'History of Syracuse, New York', u\"1939 New York World's Fair\", u'Vanity Fair (magazine)', u'Big Bend Gold Rush', u'Crazy on You', u\"1924\\u201325 St. John's Redmen basketball team\", u'Granada Television', u'State highway', u'Cassiar Gold Rush', u'Columbia Gorge', u'Bad bank', u'Public limited company', u'1946 National Invitation Tournament', u'NIT all-time team records', u'Starting pitcher', u'Android (operating system)', u\"3200 series (Chicago 'L')\", u'2003 Atlanta Braves season', u'Crescent, New York', u'Ultra vires', u'GNU Scientific Library', u'Chatham University', u'Banking', u'Limited liability partnership', u'1976 National Invitation Tournament', u'7d (New York City Subway service)', u'Brennan Motor Manufacturing Company', u'Wrigley Field', u'One Shining Moment', u'Joralemon Street Tunnel', u'Jim Marshall (American football)', u'Finnish banking crisis of the 1990s', u'Kansallis-Osake-Pankki', u'1981 NIT', u'American Bridge Company', u'Bank regulation', u'Free Software Foundation Europe', u'MBTA', u'Golf World', u'Henri Soul\\xe9', u'National Register of Historic Places listings in Saratoga County, New York', u'Fon people', u'List of Chicago Bears award winners', u'Edgemere, Queens', u'New York City Department of Transportation', u'Banking license', u'Ijaw people', u'Blue Line (CTA)', u'1897 New York Giants season', u'St. Albans, Queens', u'1958 Chicago Bears season', u'IND Eighth Avenue Line', u'Massachusetts Route 128', u'East Los Angeles (region)')\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Now let's look at the names of the pages we got.'''\n",
      "\n",
      "# Use zip(*) to get the links and contents out of the pages dictionary\n",
      "# print out the links just to see what you got "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Looks like some really interesting stuff. \n",
      "\n",
      "Onward, to the LSI. \n",
      "\n",
      "Let's preprocess and TFIDF-Vectorize our data. We will reuse the preprocess function from the earlier homework, which I have provided here.''' \n",
      "\n",
      "import re \n",
      "import string \n",
      "\n",
      "def preprocess(text): \n",
      "    text = re.sub(r'[%s]' % string.punctuation, '', text.lower())\n",
      "    return text\n",
      "\n",
      "corpus = map(preprocess, raw_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use TFIDF-Vectorizer or CountVectorizer (or both!) to vectorize the documents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''And now the LSI. This will embed the documents in a lower dimensional, conceptual space.'''\n",
      "\n",
      "# Compute the SVD (using TruncatedSVD) of the vectorized corpus\n",
      "N_COMPONENTS = 100  # feel free to change this if you so desire; typical values are between 50 and 400"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Now let's use MiniBatchKMeans to cluster these documents (or, I should say, their representations in the \n",
      "lower dimensional space).'''\n",
      "\n",
      "# Cluster with MiniBatchKMeans\n",
      "N_CLUSTERS = 10  # feel free to experiment with this value, too"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[5 5 1 9 3 5 5 2 1 5 7 2 9 8 8 4 5 1 9 5 5 6 5 4 1 9 7 6 5 8 7 5 2 7 9 8 9\n",
        " 6 5 0 2 5 8 8 8 9 5 5 5 6 7 3 5 0 6 6 2 2 3 3 1 2 5 5 9 2 5 5 2 2 3 5 2 8\n",
        " 3 9 8 2 2 3 5 2 2 9 7 5 5 2 8 9 9 2 2 9 1 5 8 9 0 8]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Let's see if these clusters are meaningful by looking at all the texts that got clustered together.'''\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "clusters_to_content = defaultdict(list)\n",
      "\n",
      "for subject, page, label in zip(links, corpus, clusters): \n",
      "    clusters_to_content[label].append(subject)\n",
      "\n",
      "for idx in clusters_to_content: \n",
      "    print(idx)\n",
      "    print(clusters_to_content[idx])\n",
      "    print('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "[u'Pennsylvania Route 981', u'State highway', u'Massachusetts Route 128']\n",
        "\n",
        "\n",
        "1\n",
        "[u'1943 Boston Braves season', u'1943 New York Giants (MLB) season', u'1890 Cincinnati Reds season', u'1894 Cincinnati Reds season', u'Starting pitcher', u'1897 New York Giants season']\n",
        "\n",
        "\n",
        "2\n",
        "[u'Centaur Media', u'Associated British Corporation', u'Century Motor Vehicle Company', u'Bricolage Production Company', u'Bad bank', u'Public limited company', u'Android (operating system)', u'Ultra vires', u'Banking', u'Limited liability partnership', u'Brennan Motor Manufacturing Company', u'Finnish banking crisis of the 1990s', u'Kansallis-Osake-Pankki', u'Bank regulation', u'Free Software Foundation Europe', u'Fon people', u'Banking license', u'Ijaw people']\n",
        "\n",
        "\n",
        "3\n",
        "[u'1987 National Invitation Tournament', u\"1924\\u201325 St. John's Redmen basketball team\", u'1946 National Invitation Tournament', u'NIT all-time team records', u'1976 National Invitation Tournament', u'One Shining Moment', u'1981 NIT']\n",
        "\n",
        "\n",
        "4\n",
        "[u'Airport Transit System', u'African people']\n",
        "\n",
        "\n",
        "5\n",
        "[u'Mauri Honkajuuri', u'University of Scranton', u'Madison Square Garden', u'Creative Review', u'Itv.com', u'ITV Emergency National Service', u'Independent Television Service for Wales and the West', u'Platform screen doors', u'National Register of Historic Places listings in Peekskill, New York', u'Channel Islands of California', u'Xavier University (Cincinnati)', u'Queensboro Hill, Queens', u'Subway Series', u'History of Syracuse, New York', u\"1939 New York World's Fair\", u'Vanity Fair (magazine)', u'Granada Television', u\"3200 series (Chicago 'L')\", u'2003 Atlanta Braves season', u'GNU Scientific Library', u'Chatham University', u'7d (New York City Subway service)', u'American Bridge Company', u'Henri Soul\\xe9', u'National Register of Historic Places listings in Saratoga County, New York', u'St. Albans, Queens']\n",
        "\n",
        "\n",
        "6\n",
        "[u'Canadian', u'Cascade Range', u'South Coast (California)', u'Big Bend Gold Rush', u'Cassiar Gold Rush', u'Columbia Gorge']\n",
        "\n",
        "\n",
        "7\n",
        "[u'Keith Olsen', u'Passionworks', u'Jethro Tull (band)', u'Vogue Paris', u'Crazy on You', u'Golf World']\n",
        "\n",
        "\n",
        "8\n",
        "[u'Tom Paciorek', u'Gaylord Perry', u'1935 Chicago Bears season', u'Fred Hutchinson', u'Bill George (American football player)', u'Fog Bowl (American football)', u'Los Angeles', u'Wrigley Field', u'Jim Marshall (American football)', u'List of Chicago Bears award winners', u'1958 Chicago Bears season', u'East Los Angeles (region)']\n",
        "\n",
        "\n",
        "9\n",
        "[u'IRT Powerhouse', u'BMT Broadway Line', u'Rockaway Park Shuttle', u\"Chicago 'L'\", u'Rockaway Park, Queens', u'Jamaica Estates, Queens', u'Erie Canal', u'Crescent, New York', u'Joralemon Street Tunnel', u'MBTA', u'Edgemere, Queens', u'New York City Department of Transportation', u'Blue Line (CTA)', u'IND Eighth Avenue Line']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Experiment with the value of K (i.e., N_CLUSTERS) in K-Means, and see what value gives you the highest silhouette score \n",
      "(just use sklearn's implementation of the silhouette score).'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "\"Experiment with the value of K (i.e., N_CLUSTERS) in K-Means, and see what value gives you the highest silhouette score \\n(just use sklearn's implementation of the silhouette score).\""
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Now let's build a recommendation engine based on this! Or at least a piece of one. We will take in a document and return \n",
      "the top N most similar documents. We will use the cosine similarity as our similarity metric -- it's true that we are in a \n",
      "lower dimensional space, but in the grand scheme of things the dimensionality is quite large (much larger than 2 or 3 dimensions), \n",
      "so the cosine similarity is still the best metric.''' \n",
      "\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "\n",
      "N_MOST_SIMILAR = 10\n",
      "\n",
      "# Write a function that takes in a text document, \n",
      "# and uses the cosine similarity to find the top N most similar documents -- in the SVD-reduced, conceptual space -- to \n",
      "# that document, and returns the names of those articles (the 'links') as a list. \n",
      "\n",
      "def get_similar_docs(doc): \n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Erie Canal\n",
        "[u\"1939 New York World's Fair\", u'Queensboro Hill, Queens', u'National Register of Historic Places listings in Peekskill, New York', u'Century Motor Vehicle Company', u'Subway Series', u'BMT Broadway Line', u'New York City Department of Transportation', u'History of Syracuse, New York', u'Crescent, New York']\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Awesome work! How does it look?'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "'Awesome work! How does it look?'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Now, just as a final, interesting point, let's plot our documents. How? Make that LSI project down to 2 dimensions, \n",
      "and then plot the projections. Why? It can be useful to see the distribution of your corpus -- is it seriously skewed? -- but \n",
      "mostly it just looks pretty.''' \n",
      "\n",
      "# So that the plot below works, set the 2D embedding of your corpus (i.e., the result of LSI) to be named embedded_2d_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'TruncatedSVD' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-7c61fffbf889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m mostly it just looks pretty.''' \n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtsvd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0membedded_2d_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsvd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'TruncatedSVD' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt \n",
      "%matplotlib inline  # this will make our plots show up when we build them\n",
      "\n",
      "# matplotlib makes us plot y-values vs x-values, \n",
      "# so we put the second column of embedded_2d_corpus in the y-values \n",
      "# and we put the first column in the x-values\n",
      "plt.plot(embedded_2d_corpus[:,1], embedded_2d_corpus[:,0], 'bo')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'embedded_2d_corpus' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-4e37a310851b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_2d_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_2d_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'embedded_2d_corpus' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''And that's what a small sample of Wikipedia looks like in two dimensions!'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}