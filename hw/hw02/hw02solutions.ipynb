{
 "metadata": {
  "name": "",
  "signature": "sha256:9eb8e7167b9a2a33ce73ad0a78123dffebacb80d3fed6045a9ef632c7da899d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Welcome to Homework 2.\n",
      "\n",
      "To start, we will do a short exercise designed to test your knowledge of basic natural\n",
      "language processing (NLP) techniques in Python. It is adapted from my former \n",
      "colleague Sam Brotherton's questions. \n",
      "\n",
      "In each of the parts below, I have provided a function definition (with the\n",
      "correct arguments but no implementation) and some tests that will pass *if* \n",
      "you fill in the correct implementation for each function. If you run this and it does not throw\n",
      " any errors, then you have finished the exercise!\n",
      "'''\n",
      "# Question 1: Normalization and Tokenization\n",
      "# Normalize and tokenize the text. \n",
      "import re \n",
      "import string\n",
      "\n",
      "def process_text(text):\n",
      "    text = re.sub(r'[%s]' % string.punctuation, '', text.lower())\n",
      "    return text.split()\n",
      "    \n",
      "# TESTS\n",
      "assert process_text('Python is SO AWESOME!!!!!! YAy!!!!@ I love programming in python!') == ['python', 'is', 'so', 'awesome', 'yay', 'i', 'love', 'programming', 'in', 'python']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 2: Count word occurences\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "def count_words(text):\n",
      "    processed = process_text(text)\n",
      "    return Counter(processed)\n",
      "    \n",
      "# TESTS\n",
      "assert count_words('Python is SO AWESOME!!!!!! YAy!!!!@ I love programming in python!') == {'yay': 1, 'python': 2, 'is': 1, 'programming': 1, 'i': 1, 'so': 1, 'in': 1, 'love': 1, 'awesome': 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 3: Create a string distance function. \n",
      "# For this, you should use the cosine similarity. Note that since a distance \n",
      "# is a *dissimilarity* measure, you will need to negate the cosine similarity \n",
      "# (thus making it the \"cosine dissimilarity\") in order for it to work as a distance function.\n",
      "\n",
      "import math \n",
      "\n",
      "def distance(text1, text2):\n",
      "    # remember that sparse vectors and matrices can be represented as dictionaries\n",
      "    counts1, counts2 = count_words(text1), count_words(text2)\n",
      "    # compute dot product of texts \n",
      "    dot_product = 0\n",
      "    terms = set(counts1.keys() + counts2.keys())\n",
      "    for term in terms: \n",
      "        dot_product += counts1.get(term, 0)*counts2.get(term, 0)\n",
      "    # compute L2-norm of each text\n",
      "    norm1 = math.sqrt(sum(map(lambda x: x**2, counts1.values())))\n",
      "    norm2 = math.sqrt(sum(map(lambda x: x**2, counts2.values())))\n",
      "    # map the numbers to floats so that we avoid integer-divsion, rounding-to-zero errors\n",
      "    [dot_product, norm1, norm2] = map(float, [dot_product, norm1, norm2])\n",
      "    # compute similarity and negate it to make it dissimilarity\n",
      "    cosine_similarity = dot_product/(norm1*norm2)\n",
      "    return -cosine_similarity\n",
      "\n",
      "# TESTS\n",
      "assert distance('I love my mom', 'i love my daddy') < distance('I love my mom', 'I am a big boy now')\n",
      "assert distance('some strings are similar to other strings', 'some strings') > distance('some strings are similar to other strings', 'some strings are similar')\n",
      "assert distance('i hate hate hate noodles.', 'i hate hate noodles') < distance('i hate hate hate noodles.', 'i hate noodles.')\n",
      "\n",
      "print 'Success! All tests passed!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Success! All tests passed!\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Now let's move on to linear algebra and linear regression. \n",
      "\n",
      "We will build our own linear regression model on the Boston home prices dataset.\n",
      "'''\n",
      "\n",
      "# Question 4: Write a function that returns the product of two matrices, given as numpy arrays. \n",
      "# Using numpy shortcuts is okay this time. Even very, very short ones :)\n",
      "import numpy as np\n",
      "\n",
      "def matrix_multiply(A, B): \n",
      "    return np.dot(A, B)\n",
      "\n",
      "attempt = matrix_multiply(np.arange(15).reshape((3,5)), np.arange(15).reshape(5,3))\n",
      "truth = np.array([[ 90, 100, 110],\n",
      "              [240, 275, 310],\n",
      "              [390, 450, 510]])\n",
      "\n",
      "np.testing.assert_array_equal(attempt, truth)\n",
      "\n",
      "attempt = matrix_multiply(np.eye(5), np.eye(5))\n",
      "truth = np.eye(5)\n",
      "\n",
      "np.testing.assert_array_equal(attempt, truth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 5: Write a function that returns the inverse of a matrix, given as a numpy array. \n",
      "# If the matrix given is not invertible, this function should return the pseudoinverse of the matrix.\n",
      "# Again, for this question, using numpy shortcuts is okay. \n",
      "def matrix_invert(A): \n",
      "    try: \n",
      "        return np.linalg.inv(A)\n",
      "    except: \n",
      "        return np.linalg.pinv(A)\n",
      "\n",
      "# if it worked at all, you probably did it right \n",
      "assert matrix_invert(np.arange(16).reshape(4,4)).shape == (4,4)\n",
      "assert matrix_invert(np.random.randn(4,4)).shape == (4,4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's load the Boston home prices dataset from scikit-learn and read the description.  \n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "beantown = load_boston()\n",
      "print beantown.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Boston House Prices dataset\n",
        "\n",
        "Notes\n",
        "------\n",
        "Data Set Characteristics:  \n",
        "\n",
        "    :Number of Instances: 506 \n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive\n",
        "    \n",
        "    :Median Value (attribute 14) is usually the target\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        - CRIM     per capita crime rate by town\n",
        "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        - INDUS    proportion of non-retail business acres per town\n",
        "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        - NOX      nitric oxides concentration (parts per 10 million)\n",
        "        - RM       average number of rooms per dwelling\n",
        "        - AGE      proportion of owner-occupied units built prior to 1940\n",
        "        - DIS      weighted distances to five Boston employment centres\n",
        "        - RAD      index of accessibility to radial highways\n",
        "        - TAX      full-value property-tax rate per $10,000\n",
        "        - PTRATIO  pupil-teacher ratio by town\n",
        "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "        - LSTAT    % lower status of the population\n",
        "        - MEDV     Median value of owner-occupied homes in $1000's\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
        "\n",
        "This is a copy of UCI ML housing dataset.\n",
        "http://archive.ics.uci.edu/ml/datasets/Housing\n",
        "\n",
        "\n",
        "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "\n",
        "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
        "prices and the demand for clean air', J. Environ. Economics & Management,\n",
        "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
        "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
        "pages 244-261 of the latter.\n",
        "\n",
        "The Boston house-price data has been used in many machine learning papers that address regression\n",
        "problems.   \n",
        "     \n",
        "**References**\n",
        "\n",
        "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
        "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
        "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 6: separate the data into features and targets, and split into training and testing sets.\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X, y = beantown.data, beantown.target \n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 7: Use the normal equations (those matrix equations we learned in class\n",
      "# that solve for the optimal weights in linear regression) to find the optimal weight\n",
      "# vector w. \n",
      "# We will use this to build a linear model to predict a target price in Boston for a given home. \n",
      "\n",
      "def solve(X, y): \n",
      "    # remember to add in the bias term! \n",
      "    X = np.hstack([np.ones(X.shape[0]).reshape((X.shape[0],1)), X])\n",
      "    sub1 = matrix_invert(matrix_multiply(X.T, X))\n",
      "    sub2 = matrix_multiply(X.T, y)\n",
      "    w = matrix_multiply(sub1, sub2)\n",
      "    return w\n",
      "\n",
      "def solve2(X, y): \n",
      "    X = np.hstack([np.ones(X.shape[0]).reshape((X.shape[0],1)), X])\n",
      "    sub1 = np.linalg.pinv(np.dot(X.T, X))\n",
      "    sub2 = np.dot(X.T, y)\n",
      "    w = np.dot(sub1, sub2)\n",
      "    return w\n",
      "\n",
      "w = solve(X_train, y_train)\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  3.51149463e+01  -7.74000154e-02   3.91312920e-02   3.34968554e-02\n",
        "   3.35947780e+00  -1.61486436e+01   3.81256330e+00  -4.09390052e-03\n",
        "  -1.43780726e+00   3.04966792e-01  -1.36750609e-02  -8.88112759e-01\n",
        "   1.04912565e-02  -5.80664978e-01]\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 8: Get predictions from your model\n",
      "# Note: You will have to think hard about (and maybe do some interactive debugging of...)\n",
      "# the shapes of your matrices. \n",
      "def get_preds(w, test_set_X): \n",
      "    test_set_X = np.hstack([np.ones(test_set_X.shape[0]).reshape((test_set_X.shape[0],1)), test_set_X])\n",
      "    return np.dot(w, test_set_X.T)\n",
      "\n",
      "preds1 = get_preds(w, X_test)\n",
      "\n",
      "# visually inspect your predictions as a sanity check -- are they in the ballpark of what they should be? \n",
      "print preds1 \n",
      "print y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 20.30944979  39.46736411  28.10828871  17.58876651  21.5580466\n",
        "  21.8536957   11.64698494  37.27188115  30.28143931  30.78246197\n",
        "  29.05830693   3.6034796   30.35182371  34.03878575  31.1102339\n",
        "  19.73884752  10.15324251  27.66076101  13.43023119  18.3763734\n",
        "  45.03748983  17.93257401  22.356551     2.90084026  42.78609968\n",
        "  22.27580026  25.72865281  35.77883653   2.55748869  32.47332815\n",
        "  19.28791971  25.15297805  31.78435135  22.18581881  30.27456668\n",
        "  23.0233533   34.51466887  22.36274863  19.56188902  19.80185718\n",
        "  19.53088146  20.14486037  24.46616559  25.23371915  26.44221877\n",
        "  17.70033433  22.7253403   19.87437568  22.34078016  11.89741527\n",
        "  22.52098719  16.78788133  12.04747901  16.44553409  20.62986929\n",
        "  15.47199283  21.01881523  22.70085271  26.64148366  32.91127865\n",
        "   0.24179426  25.58722874   7.95120405  33.38497762  27.59823533\n",
        "  11.57735486   4.92694359   9.30333848  15.55961591  12.09283969\n",
        "  23.6814384   25.004799    12.65084313  20.46597336  30.29151662\n",
        "  29.42589719  19.99017277  22.18721897  28.68403731  24.37168289\n",
        "  20.55852908  33.75076717  33.00549435  25.75785653  25.76139804\n",
        "  33.32202271  22.73457701  15.89957098  20.689825    35.55485742\n",
        "  16.58154211  16.59426795  23.96158757  23.23334782  27.00398305\n",
        "  40.66038562  23.82262316  29.27026507  21.96001282  15.91103121\n",
        "  20.44010404  31.64494186]\n",
        "[ 16.8  43.5  26.6  16.6  19.8  18.7  16.3  44.   32.9  25.1  22.9   8.4\n",
        "  23.   32.   32.2  20.5  16.5  23.7  13.3  20.8  50.   12.7  19.3  14.4\n",
        "  50.   21.1  28.7  33.1   8.8  29.   20.3  25.   31.5  23.3  24.8  22.6\n",
        "  35.4  26.6  20.1  19.9  18.2  13.8  26.2  24.   22.6  19.6  20.3  20.4\n",
        "  21.2  11.8  23.1  20.6  13.4  20.2  19.5  15.6  21.   21.7  22.   33.1\n",
        "  17.9  15.   14.4  41.3  27.1  15.6  13.8   9.7  15.6  13.6  21.7  50.\n",
        "  13.4  20.   32.7  25.   27.9  22.5  24.4  22.6  15.6  26.7  32.   24.2\n",
        "  21.6  37.2  20.6  10.2  21.7  33.4  10.2  10.4  18.9  19.4  22.3  50.\n",
        "  22.2  33.4  21.4  13.8  21.4  29. ]\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 9: To evaluate our model, let's write a function \n",
      "# to calculate the mean squared error between an array of predictions and the truth array. \n",
      "\n",
      "def mse(truth, predictions): \n",
      "    # we know it's a vector, but we don't know row or column; so we take it's length to \n",
      "    length = float(max(predictions.shape))\n",
      "    squared_errors = map(lambda (t, p): (t - p)**2, zip(truth, predictions))\n",
      "    return sum(squared_errors)/length\n",
      "    \n",
      "assert mse(np.array([1,2,3]), np.array([1,2,3])) == 0.\n",
      "assert mse(np.array([1,1,1]), np.array([1,4,5])) == 25./3. \n",
      "\n",
      "# Now let's evaluate\n",
      "err1 = mse(preds1, y_test)\n",
      "print err1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 10: Doing it by hand is instructive, but let's see what scikit-learn would have gotten us. \n",
      "# Use scikit-learn to train a linear model. \n",
      "\n",
      "from sklearn.linear_model import LinearRegression \n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "linreg = LinearRegression()\n",
      "linreg.fit(X_train, y_train)\n",
      "preds2 = linreg.predict(X_test)\n",
      "err2 = mean_squared_error(y_test, preds2)\n",
      "print preds2\n",
      "print err2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 20.30944979  39.46736411  28.10828871  17.58876651  21.5580466\n",
        "  21.8536957   11.64698494  37.27188115  30.28143931  30.78246197\n",
        "  29.05830693   3.6034796   30.35182371  34.03878575  31.1102339\n",
        "  19.73884752  10.15324251  27.66076101  13.43023119  18.3763734\n",
        "  45.03748983  17.93257401  22.356551     2.90084026  42.78609968\n",
        "  22.27580026  25.72865281  35.77883653   2.55748869  32.47332815\n",
        "  19.28791971  25.15297805  31.78435135  22.18581881  30.27456668\n",
        "  23.0233533   34.51466887  22.36274863  19.56188902  19.80185718\n",
        "  19.53088146  20.14486037  24.46616559  25.23371915  26.44221877\n",
        "  17.70033433  22.7253403   19.87437568  22.34078016  11.89741527\n",
        "  22.52098719  16.78788133  12.04747901  16.44553409  20.62986929\n",
        "  15.47199283  21.01881523  22.70085271  26.64148366  32.91127865\n",
        "   0.24179426  25.58722874   7.95120405  33.38497762  27.59823533\n",
        "  11.57735486   4.92694359   9.30333848  15.55961591  12.09283969\n",
        "  23.6814384   25.004799    12.65084313  20.46597336  30.29151662\n",
        "  29.42589719  19.99017277  22.18721897  28.68403731  24.37168289\n",
        "  20.55852908  33.75076717  33.00549435  25.75785653  25.76139804\n",
        "  33.32202271  22.73457701  15.89957098  20.689825    35.55485742\n",
        "  16.58154211  16.59426795  23.96158757  23.23334782  27.00398305\n",
        "  40.66038562  23.82262316  29.27026507  21.96001282  15.91103121\n",
        "  20.44010404  31.64494186]\n",
        "25.5395430487\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Final test: is your hand-built model's error equal to scikit-learn's model's error up to 4 digits? \n",
      "\n",
      "assert round(err1, 4) == round(err2, 4)\n",
      "\n",
      "print 'Success! You just built a full linear regression model!!!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Success! You just built a full linear regression model!!!\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}